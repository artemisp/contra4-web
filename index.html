<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Evaluating mathematical reasoning of foundation models in visual contexts">
  <meta name="keywords" content="MathVista, Math Vista">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title> DisCRn: Evaluating Discriminatory Cross-Modal Reasoning in Audio, Video, Image, and 3D</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
/Users/panlu/Library/Mobile Documents/com~apple~CloudDocs/ImageMath/visual-mathqa-server/data_final/images
    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link rel="icon" href="./static/images/dalee_logo.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <!-- <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bulma.min.css" rel="stylesheet">
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script> -->
  <script type="text/javascript" src="static/js/sort-table.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>

  <script src="https://www.youtube.com/iframe_api"></script>


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <script src="./static/js/leaderboard_testmini.js"></script>  
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>
</head>
<body>
<!-- 
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a> 
     @PAN TODO: consider adding links? 
      <div class="navbar-item has-dropdown is-hoverable"> 
        <a class="navbar-link">
          More Research
        </a> 
       <div class="navbar-dropdown">
          <a class="navbar-item" href="https://chameleon-llm.github.io/">
            <b>Chameleon</b> <p style="font-size:18px; display: inline; margin-left: 5px;">🔥</p>
          </a>
          <a class="navbar-item" href="https://scienceqa.github.io/">
            <b>ScienceQA</b> <p style="font-size:18px; display: inline; margin-left: 5px;">🔥</p>
          </a>
          <a class="navbar-item" href="https://github.com/OpenGVLab/LLaMA-Adapter">
            <b>LLaMA-Adapter (V2)</b> <p style="font-size:18px; display: inline; margin-left: 5px;">🔥</p>
          </a>
          <a class="navbar-item" href="https://promptpg.github.io/">
            PromptPG
          </a>
          <a class="navbar-item" href="https://arxiv.org/abs/2307.10635">
            SciBench
          </a>
          <a class="navbar-item" href="https://arxiv.org/abs/2305.12524">
            TheoremQA
          </a>
          <a class="navbar-item" href="https://lila.apps.allenai.org/">
            Lila
          </a>
          <a class="navbar-item" href="https://iconqa.github.io/">
            IconQA
          </a>
          <a class="navbar-item" href="https://lupantech.github.io/inter-gps/">
            Inter-GPS
          </a>
        </div> 
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
            <img src="static/images/dalee_logo.png" style="width:1em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista" style="vertical-align: middle">Contra4</span>
            </h1>
          <h2 class="subtitle is-3 publication-subtitle">
            Evaluating Contrastive Cross-Modal Reasoning in Audio, Video, Image, and 3D
            <!-- <br> -->
            <!-- with GPT-4V, Bard, and Other Large Multimodal Models -->
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://artemisp.github.io/">Artemis Panagopoulou</a><sup style="color:#6fbf73;">1</sup><sup style="color:#ed4b82;">*</sup>,</span>
            <span class="author-block">
              <a href="">Le Xue</a><sup style="color:#ed4b82">2</sup>,</span>
            <span class="author-block">
              <a href="">Honglu Zhou</a><sup style="color:#ed4b82">2</sup>,
            </span>
            <span class="author-block">
              <a href="">Silvio Savarese</a><sup style="color:#ed4b82">2</sup>,
            </span>
            <span class="author-block">
              <a href="">Ran Xu</a><sup style="color:#ed4b82">2</sup>,
            </span>
            <span class="author-block">
              <a href="https://chunyuan.li/">Caiming Xiong</a><sup style="color:#ed4b82">2</sup>,
            </span>
            <span class="author-block">
              <a href="">Chris Callison-Burch</a><sup style="color:#6fbf73;">1</sup>,
            </span>
            <span class="author-block">
              <a href="">Mark Yatskar</a><sup style="color:#6fbf73;">1</sup>,
            </span>
            <span class="author-block">
              <a href="">Juan Carlos Niebles</a><sup style="color:#ed4b82">2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup style="color:#6fbf73;">1</sup>University of Pennsylvania,</span><br>
            <span class="author-block"><sup style="color:#ed4b82">2</sup>Salesforce Research</span><br>
            <span class="author-block"><sup style="color:#ed4b82">*</sup>Work done during internship at Salesforce.</span>
            <!-- <span class="paper-block"><b style="color:#f41c1c">ICLR 2024 Oral</b> (85 in 7304, 1.2%)</span> -->
          </div>
        
          <!-- <section> -->
            <!-- <div class="section" id="org-banners" style="display:fle">
              <a href="https://www.ucla.edu/" target="_blank" rel="external">
                  <img class="center-block org-banner" src="static/images/ucla.png" style="height:3em">
              </a>
              <a href="https://www.washington.edu/" target="blank" class="ext-link">
                  <img class="center-block org-banner" src="static/images/uw.png" style="height:3em">
              </a>
              <a href="https://www.microsoft.com/en-us/research/" target="_blank" rel="external">
                  <img class="center-block org-banner" src="static/images/microsoft.png" style="height:3em">
              </a>
            </div> -->
          <!-- </section> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                @PAN TODO: change links
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">🤗</p>
                  </span>
                  <span>Dataset</span>
                </a>
              </span> -->
              <!-- Visualization Link. -->
              <span class="link-block">
                <a href="https://artemisp.github.io/contra4-web/#visualization"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">🔮</p>
                  </span>
                  <span>Visualize</span>
                </a>
              </span>
              <!-- Leaderboard Link. -->
              <span class="link-block">
                <a href="https://artemisp.github.io/contra4-web/#leaderboard"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">🏆</p>
                  </span>
                  <span>Leaderboard</span>
                </a>
              </span>
              <!-- Twitter Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">🌐</p>
                  </span>
                  <span>Twitter</span>
                </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container" style="margin-top: -150px; margin-bottom: -100px;">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/evolution.png" alt="geometric reasoning" width="84%"/>
              <p> 
              <img src="static/images/dalee_logo.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
              <span class="mathvista">Contra4</span>
              evaluates cross-modal models across various modalities concurrently, unlike any previous work. 
              </p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/radar_plot_pretty.png" alt="geometric reasoning" width="84%"/>
              <p> Accuracy scores of three recent cross-modal models (i.e. X-InstructBLIP, CREMA, and OneLLM) on our proposed dataset
                <img src="static/images/dalee_logo.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><span class="mathvista">Contra4</span>
              across different subsets of the annotated test set. </b>
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            To achieve a deeper understanding of the world, AI must be able to reason across multiple modalities, such as images, audio, video, and 3D. While recent efforts have extended multimodal models to process multiple modalities, there is little evidence that they enable reasoning beyond two modalities simultaneously. This limitation arises partly from the challenge of constructing tasks that require reasoning across multiple modalities. 
          </p>
          <p>
            To address this, we introduce  <img src="static/images/dalee_logo.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><b>Contra4</b>, a dataset designed to train and evaluate contrastive cross-modal reasoning over up to four modalities (audio, video, image, and 3D) simultaneously. Our approach unifies modalities through human-annotated captions and generates contrastive question-answer pairs, filtered via a mixture-of-models round-trip-consistency check.
          </p>
          <p>
            Human inspection validates the high quality of Contra4, with 83.3% perceived correctness, while fine-tuning on the task results in a 56% relative accuracy improvement. Benchmarking against state-of-the-art models on a human annotated subset of 2.3k samples underscores the dataset’s challenge, with the best-performing model achieving only 56% accuracy on the full dataset and just 42% in four-modality settings.          </p>          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>

<section class="section">
  <div class="container">
    
    <div class="columns is-centered">
      <div class="column is-full has-text-centered content">

        <h2 class="title is-3" id="leaderboard_test">Leaderboard </h2>
        <div class="content">
          <p class="mt-3">Accuracy scores on human annotated test set of <img src="static/images/dalee_logo.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista">Contra4</span>.
          </p>
          <div style="overflow-x:scroll;">
            <table class="js-sort-table" id="results">
              <tr>
                  <td class="js-sort-number"><strong>#</strong></td>
                  <td class="js-sort-number"><strong>Model</strong></td>
                  <td class="js-sort-number"><strong>Method</strong></td>
                  <td class="js-sort-number"><strong>LLM Base</strong></td> 
                  <td class="js-sort-number"><strong><u>ALL</u></strong></td>
                  <td class="js-sort-number"><strong>Random All</strong></td>
                  <td class="js-sort-number"><strong>Random MC2</strong></td>
                  <td class="js-sort-number"><strong>Random MC3</strong></td>
                  <td class="js-sort-number"><strong>Random MC4</strong></td>
                  <td class="js-sort-number"><strong>Similarity All</strong></td>
                  <td class="js-sort-number"><strong>Similarity MC2</strong></td>
                  <td class="js-sort-number"><strong>Similarity MC3</strong></td>
                  <td class="js-sort-number"><strong>Similarity MC4</strong></td>
              </tr>
                <tr>
                  <td class="js-sort-number"><strong>#</strong></td>
                  <td class="js-sort-number"><strong>Model</strong></td>
                  <td class="js-sort-number"><strong>Method</strong></td>
                  <td class="js-sort-number"><strong>LLM Base</strong></td> 
                  <td class="js-sort-number"><strong><u>ALL</u></strong></td>
                  <td class="js-sort-number"><strong>Random All</strong></td>
                  <td class="js-sort-number"><strong>Random MC2</strong></td>
                  <td class="js-sort-number"><strong>Random MC3</strong></td>
                  <td class="js-sort-number"><strong>Random MC4</strong></td>
                  <td class="js-sort-number"><strong>Similarity All</strong></td>
                  <td class="js-sort-number"><strong>Similarity MC2</strong></td>
                  <td class="js-sort-number"><strong>Similarity MC3</strong></td>
                  <td class="js-sort-number"><strong>Similarity MC4</strong></td>
                </tr>
                
                <tr>
                  <td>1</td>
                  <td><b class="best-score-text">CREMA 🥇<sup>†</sup></b></td>
                  <td>MLLM 🖼️</td>
                  <td>FlanT5-xl</td>
                  <td><strong>0.56</strong></td>
                  <td><strong>0.60</strong></td>
                  <td><strong>0.71</strong></td>
                  <td><strong>0.61</strong></td>
                  <td><strong>0.45</strong></td>
                  <td><strong>0.53</strong></td>
                  <td><strong>0.64</strong></td>
                  <td><strong>0.55</strong></td>
                  <td><strong>0.39</strong></td>
                </tr>
                
                <tr>
                  <td>3</td>
                  <td>OneLLM-Finetuned</td>
                  <td>MLLM 🖼️</td>
                  <td>LLaMA-2 7B-Finetuned</td>
                  <td><u>0.50</u></td>
                  <td><u>0.54</u></td>
                  <td>0.60</td>
                  <td>0.43</td>
                  <td><u>0.58</u></td>
                  <td><u>0.47</u></td>
                  <td>0.60</td>
                  <td><u>0.36</u></td>
                  <td>0.43</td>
                </tr>
                
                <tr>
                  <td>2</td>
                  <td>X-InstructBLIP</td>
                  <td>MLLM 🖼️</td>
                  <td>Vicuna1.1 7B</td>
                  <td>0.32</td>
                  <td>0.31</td>
                  <td>0.47</td>
                  <td>0.30</td>
                  <td>0.13</td>
                  <td>0.33</td>
                  <td>0.48</td>
                  <td>0.27</td>
                  <td>0.22</td>
                </tr>
                
                <tr>
                  <td>3</td>
                  <td>OneLLM</td>
                  <td>MLLM 🖼️</td>
                  <td>LLaMA-2 7B</td>
                  <td>0.32</td>
                  <td>0.31</td>
                  <td>0.52</td>
                  <td>0.16</td>
                  <td>0.24</td>
                  <td>0.34</td>
                  <td>0.52</td>
                  <td>0.22</td>
                  <td><u>0.27</u></td>
                </tr>
                
                <tr>
                  <td>4</td>
                  <td>Gemini-2.0<sup><b>*</b></sup></td>
                  <td>MLLM 🖼️</td>
                  <td>gemini-2.0-flash-exp</td>
                  <td>0.22</td>
                  <td>0.23</td>
                  <td>0.24</td>
                  <td>0.10</td>
                  <td>&times;</td>
                  <td>0.20</td>
                  <td>0.21</td>
                  <td>0.14</td>
                  <td>&times;</td>
                </tr>
                
                <tr>
                  <td>5</td>
                  <td>Predicted Caption</td>
                  <td>LLM 💬</td>
                  <td>LLaMA-3.1 7B</td>
                  <td>0.37</td>
                  <td>0.38</td>
                  <td><u>0.52</u></td>
                  <td><u>0.33</u></td>
                  <td>0.26</td>
                  <td><u>0.36</u></td>
                  <td><u>0.46</u></td>
                  <td>0.33</td>
                  <td>0.27</td>
                </tr>
      </table>
    </div>

          <!-- <b>Human Performance*:</b> Average human performance from AMT annotators who have high school diplomas or above. -->
          <br>
          <b>Method types:</b> <b>MLLM 🖼️:</b> Cross-Modal model, <b>LLM 💬:</b> Large Language Model with Predicted Captions
          <br>
          <b>MCX:</b> Multiple choice with X options. <b>Similarity:</b> Negative sampling based on high similarity of captions. <b>Random:</b> Random negative sampling. 
          <br>
          <sup>†</sup> CREMA uses additional RGB signal for 3D inputs.
          <br>
          <b>*</b> Gemini is evaluated in examples that do not include 3D since it does not support 3D input as of yet. 
          <br>
          <br>
          <div>
          <p>🚨 To submit your results to the leaderboard, please send to <a href="mailto:artemisp@seas.upenn.edu">this email</a> with your result json files.</p>
          <!-- <p>🚨 For more submission details, please refer to <a href="https://github.com/lupantech/MathVista?tab=readme-ov-file#-leaderboard-">this link</a> and <a href="https://github.com/lupantech/MathVista?tab=readme-ov-file#-evaluations-on-mathvista">this link</a>. -->
          </p>
          </div>
        </div>

      </div>
    </div>

  </div>
</section>

<!-- DATASET SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <!-- <h1 class="title is-1 mathvista"><img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
  <h1 class="title is-1 mathvista">
    <img src="static/images/dalee_logo.png" style="width:1em;vertical-align: middle" alt="Logo"/>
    <span class="mathvista" style="vertical-align: middle">Contra4 Dataset</span>
  </h1>
  </div>
</section>

<!-- <section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p> -->
            
<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <!-- <div class="column is-full-width has-text-centered"> -->
        <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            <img src="static/images/dalee_logo.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista">Contra4</span> dataset introduced in this paper comprises <b>174k</b> distinct training samples and <b>2.3k</b> human-annotated test samples, designed to evaluate the cross-modal reasoning capabilities of large multimodal models.
            The inputs span up to <b>four different modalities</b> across audio, video, image and 3D sourced from <b>5 different captioning datasets</b> using a mixture-of-models round-trip-consistency method to generate questions and verify answers.
            A human inspection of the data showed that our data generation pipeline achieves <b>83.3% accuracy</b> in generating valid examples for fine-tuning.
          </p>

          <div id="results-carousel" class="carousel results-carousel">
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/examples1.png" alt="examples" width="100%"/>
                <p> Examples across different categories </p>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/examples2.png" alt="examples" width="100%"/>
                <p> Examples across different categories </p>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/examples3.png" alt="examples" width="100%"/>
                <p> Examples across different categories</p>
              </div>
            </div>

            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/examples4.png" alt="examples" width="100%"/>
                <p> Examples across different categories</p>
              </div>
            </div>
          </div>

        </div>
      </div>
    </div>

  

    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Data Generation & Finetuning</h2>
        <p>Mixture-of-Models Round-Trip-Consistency data generation pipeline for <img src="static/images/dalee_logo.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
          <span class="mathvista">Contra4</span></p>
          <style>
            .step1 { color: rgb(226, 148, 108); }
            .step2 { color: rgb(94, 148, 211); }
            .step3 { color: rgb(110, 40, 107); }
            .step4 { color: rgb(130, 162, 111); }
            .step-paragraphs {
              text-align: left;
              margin-left: 2em; /* Adjust for stronger or weaker indent */
            }
        </style>

        <div id="results-carousel" class="carousel results-carousel">

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/mom_rtc.png" alt="step1" width="80%"/>
              
              <div class="step-paragraphs">
              <p><strong>Pipeline Overview:</strong> We start with datasets from different modalities, each paired with captions.</p>
          
              <p class="step1">Step 1: Negative Sampling Selection – Select challenging negative examples using either similar captions or random pairing.</p>
              
              <p class="step2">Step 2: Question Generation – Use a language model to generate questions that require reasoning across multiple modalities.</p>
              
              <p class="step3">Step 3: Answer-Explanation Generation – Ask the model to answer and explain its reasoning based on the captions.</p>
              
              <p class="step4">Step 4: Round-Trip-Consistency Check - Validate each example by checking whether multiple models can reach the same answer and explanation. Only high-agreement examples are kept.</p>
              </div>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/ft_onellm_all.png" alt="finetuning" width="100%"/>
              We apply a round-trip-consistency check using multiple language models to verify each sample. Samples are retained only if they pass filters such as majority agreement (MF), unanimous agreement (UF), and their permutation-aware variants (PMF, PUF). These filtering strategies significantly boost model accuracy over training iterations especially in the earlier stages.
            </div>
          </div>
        </div>
      </div>
    </div>



    <div class="columns is-centered m-6">
      <div class="column is-max-desktop has-text-centered">
        <h2 class="title is-3" id="visualization">Visualization</h2>
        <iframe src="visualizer/explore.html" style="width: 100%;min-height: 100vh; border-radius: 20px;"></iframe>
      </div>
    </div>
  </div>
</section>

<!-- RESULTS SECTION
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 mathvista">Experiment Results</h1>
  </div>
</section> -->

<!-- <section class="section">
  <div class="container">

    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Results on Existing Foundation Models</h2> -->
        <!-- <p>One example for each reasoning skill required in <span class="mathvista">MathVista</span></p> -->
        <!-- <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-figures/score_leaderboard_gpt4v.png" alt="grade-lv" width="60%"/>
              <p>Accuracy scores of primary baselines on the testmini subset (1,000 examples) of <img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
                <span class="mathvista">MathVista</span>. 
                <br>
                Both CoT GPT-4 and PoT GPT-4 are augmented with Bard captions and OCR text.</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-figures/bard_error.png" alt="grade-lv" width="70%"/>
              <p>
                Error analysis of Bard results: (a) presents errors in answers and explanations; <br>
                (b) delves into the details of wrong explanations. <br>
                Notations: “Answer” is “Ans.”, “Explanation” is “Exp.”, “Partially Correct” is “Partial”, <br>
                and “Not applicable” refers to unanswerable or indeterminate cases.</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-figures/grade-lv.png" alt="grade-lv" width="90%"/>
              <p>Average accuracy scores across different grade levels for leading foundation models</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-figures/contexts.png" alt="contexts" width="90%"/>
              <p>Accuracy scores of leading baselines across various visual contexts</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-figures/ablation.png" alt="geometric reasoning" width="90%"/>
              <p>Average accuracy scores of LLM baselines under various visual inputs</p>
            </div>
          </div>
        </div>
      </div>
    </div> -->

    <!-- <div class="container is-full has-text-centered content m-6" id="result-table">
      <h2 class="title is-3" id="explorer">Explorer</h2>
      <p>Explore the outputs of each model on <img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
        <span class="mathvista">MathVista</span></p>
      <div class="level has-text-centered" style="position: sticky; top: 0; z-index: 20;">
        <div class="level-item box m-3" style="width: 30%; background: rgba(250, 250, 250, 1);">
          <button class="button" style="width: 100%; border: none; background: rgba(250, 250, 250, 1);" id="refresh-qids">
            <span class="icon is-large">
              <i class="fa fa-redo fa-lg" aria-hidden="true"></i>
            </span>
            <p class="title is-4 m-0">Refresh Question</p>
          </button>
        </div>
        <div class="level-item box m-3" style="width: 30%; background: rgba(250, 250, 250, 1);">
          <div class="dropdown" style="width: 100%;">
            <div class="dropdown-trigger has-text-justified" style="width: 100%; ">
              <button class="button" aria-haspopup="true" aria-controls="dropdown-menu" style="width: 100%; border: none; background: rgba(250, 250, 250, 1);">
                <p class="title m-0 is-4 dropdown-display">Multimodal Bard</p>
                <span class="icon is-large" style="position: absolute; right:0;">
                  <i class="fas fa-angle-down fa-lg" aria-hidden="true"></i>
                </span>
              </button>
            </div>
            <div class="dropdown-menu" id="dropdown-menu" role="menu" style="width:100%;">
              <div class="dropdown-content">
                <a class="dropdown-item">
                  Dropdown item
                </a>
                <a class="dropdown-item">
                  Other dropdown item
                </a>
              </div>
            </div>
          </div>
        </div>
        <div class="level-item box m-3" style="width: 30%; background: rgba(250, 250, 250, 1);">
          <div class="dropdown" style="width: 100%;">
            <div class="dropdown-trigger has-text-justified" style="width: 100%;">
              <button class="button" aria-haspopup="true" aria-controls="dropdown-menu" style="width: 100%; border: none; background: rgba(250, 250, 250, 1);">
                <p class="title m-0 is-4 dropdown-display">CoT GPT4 (Caption+OCR)</p>
                <span class="icon is-large" style="position: absolute; right:0;">
                  <i class="fas fa-angle-down fa-lg" aria-hidden="true"></i>
                </span>
              </button>
            </div>
            <div class="dropdown-menu" id="dropdown-menu" role="menu" style="width:100%;">
              <div class="dropdown-content">
                 <a class="dropdown-item">
                  Dropdown item
                </a>
                <a class="dropdown-item">
                  Other dropdown item
                </a> 
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>  -->
</section>


<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Ethics Statement</h2>
        <div class="content has-text-justified">
          <p>
            In conducting this research, we acknowledge the significant limitations and potential dangers associated with the use of Large Language Models (LLMs). One of the primary concerns is the presence of inherent biases within LLMs, which are a direct consequence of the data on which they are trained. These biases can inadvertently perpetuate harmful stereotypes and lead to discriminatory outcomes, particularly in sensitive applications. Additionally, LLMs, especially those with large parameter counts, may generate outputs that are factually incorrect or misleading, posing a risk in contexts that demand high levels of accuracy and reliability. To mitigate these risks we inspected samples of the dataset and used image sources that would limit the potential of generation of such harmful questions. However, we emphasize the importance of ongoing vigilance and the need for responsible use of these models and our dataset to prevent unintended negative consequences.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>


<!-- @PAN TODO: bibtex -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <pre><code>
      TBD
    </code></pre>
  </div>
</section>

<section>
  <div class="section" id="org-banners" style="display:flex">
    <a href="https://www.upenn.edu/" target="_blank" rel="external">
        <img class="center-block org-banner" src="static/images/University-of-Pennsylvania-Logo.png"%>
    </a>
    <a href="https://www.salesforceairesearch.com/" target="blank" class="ext-link">
        <img class="center-block org-banner" src="static/images/salesforce-research.png">
    </a>
  </div>
</section>



<footer class="footer">
  <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is website adapted from <a href="https://mathvista.github.io/">MathVista</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            The <img src="static/images/dalee_logo.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista">DisCRn</span> logo was created using <a link="https://openai.com/index/dall-e-2/">DALL•E</a> through the <a link="https://chatgpt.com/">ChatGPT (GPT-4)</a> UI on December 10, 2024. 
          </p>
        </div>
      </div>
    </div>
  <!-- </div> -->
</footer>

</body>
</html>
